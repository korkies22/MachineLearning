{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se leen los datos utilizando readcsv de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leer el archivo con los datos\n",
    "data_wine = pd.read_csv('./data/winequality-white.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se separan los datos en \"x\" y \"y\". Se normaliza x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La variable dependiente es la última columna, las independientes son las anteriores\n",
    "x= data_wine.iloc[:,0:11]\n",
    "y= data_wine.iloc[:,11:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se parten los datos en entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se divide el archivo para entrenamiento y test. Se reserven 10000 datos para test\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 2000, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se concatenan los datos de test\n",
    "newData= pd.concat([xTrain,yTrain], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se escogen aleatoriamente diferentes tamaños de datos para entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De estos datos concatenados se escogen aleatoriamente 100,1000 y 2898 para diferentes modelos. \n",
    "#De aquí se vuelven a separar en x y y\n",
    "dataTrain1= newData.sample(100)\n",
    "xTrain1= dataTrain1.iloc[:,0:11]\n",
    "yTrain1= dataTrain1.iloc[:,11]\n",
    "dataTrain2= newData.sample(1000)\n",
    "xTrain2= dataTrain2.iloc[:,0:11]\n",
    "yTrain2= dataTrain2.iloc[:,11]\n",
    "dataTrain3= newData.sample(2898)\n",
    "xTrain3= dataTrain3.iloc[:,0:11]\n",
    "yTrain3= dataTrain3.iloc[:,11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función calcula la suma de los errores cuadráticos para obtener un estimativo del error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que calcula el error cuadrático medio a partir de ciertos datos y el vector de parámetros w\n",
    "def calc_errors(x,y,w):\n",
    "    err=0\n",
    "    for i in range(0,x.shape[0]):\n",
    "        err+=(y[i]-(x[i]*w).sum())**2\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen la función necesaria para realizar descenso de gradiente estocásico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que calcula el gradiente descendente, demora al correr\n",
    "#numErrors es el número de errores consecutivos cuya media debe ser menor que el threshold \n",
    "#para que se asuma que el algoritmo terminó, este se escogió con ensayo y error.\n",
    "#El threshold se define por la variable minError, este se escogió con ensayo y error.\n",
    "def gradient_desc(nX,nY,numCiclos=29,minError=0.0001):\n",
    "    a=nX.to_numpy()\n",
    "    y=nY.to_numpy()\n",
    "    #Se concatena un 1 al final de las variables para representar la constante w0\n",
    "    x = np.ones((a.shape[0],a.shape[1]+1))\n",
    "    x[:,:-1] = a\n",
    "    wNumber= x.shape[1]\n",
    "    numData= x.shape[0]\n",
    "    #Se calcula la hessiana\n",
    "    H=[[0] * wNumber]* wNumber\n",
    "    for i in range(0, numData):\n",
    "        xi= x[i]\n",
    "        H+=xi.transpose()*xi\n",
    "    ei=np.linalg.eigvals(H)\n",
    "    #Se obtienen los valores propios y se calcula n como 20/lambdaMax, valor escogido con ensayo y error. \n",
    "    #Si se utiliza 2/lambdaMax que es lo que indica la teoría el algoritmo demora mucho en converger\n",
    "    n=20/max(ei)\n",
    "    ws= [0.01] * wNumber\n",
    "    errorA=1\n",
    "    errorP=0\n",
    "    j=0\n",
    "    #Variable que representa la media mínima del error cuadrático medio actual\n",
    "    minT=10\n",
    "    #El ciclo se ejecuta hasta que el error cuadrático medio sea similar al error cuadrático medio\n",
    "    #de los numCiclos ciclos anteriores a este. Así se asume que el modelo convergió\n",
    "    while minT > minError:\n",
    "        #Se escoge una fila de datos aleatoriamente\n",
    "        i= randrange(numData)\n",
    "        #Se calcula ws con el algoritmo de descenso de gradiente estocástico\n",
    "        g=(x[i]*ws).sum()\n",
    "        e=(g-y[i])         \n",
    "        ws= ws - n*e*x[i]\n",
    "        j=(j+1)%numCiclos\n",
    "        #Cada numCiclos ciclos se pregunta si el error se similar al error de hace numCiclos ciclos\n",
    "        if j==0:\n",
    "            errorP=errorA\n",
    "            errorA=calc_errors(x,y,ws)\n",
    "            #Se guarda solo el error mínimo, que es el que se usa para comparar\n",
    "            if abs(errorP-errorA)< minT:\n",
    "                minT=abs(errorP-errorA)\n",
    "                #Se puede descomentar el siguiente print para observar como este valor disminuye\n",
    "                #print(minT)\n",
    "    return ws\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrenan los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01829272 -0.21936836 -0.03196588  0.01971941  0.01401011  0.01028376\n",
      "  0.00121436  0.09574733  0.30762736  0.23443655  0.40377095  0.09623186]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 1\n",
    "linearRegr1 = gradient_desc(xTrain1, yTrain1)\n",
    "print(linearRegr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16758253 0.01430592 0.01651852 0.00456606 0.01070025 0.01463262\n",
      " 0.00043499 0.03601617 0.09644452 0.02214248 0.34972709 0.03633775]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 2\n",
    "linearRegr2 = gradient_desc(xTrain2, yTrain2)\n",
    "print(linearRegr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16899226  0.01395513  0.0175724   0.01360519  0.01056015  0.01292283\n",
      " -0.00108622  0.03771681  0.10248975  0.02375005  0.37098553  0.03806201]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 3\n",
    "linearRegr3 = gradient_desc(xTrain3, yTrain3)\n",
    "print(linearRegr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye una función de éxito para evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para calcular R^2 como 1-u/v, similar a como lo calcula sklearn para validar\n",
    "def calc_r2(nX,nY,w):\n",
    "    a=nX.to_numpy()\n",
    "    y=nY.to_numpy()\n",
    "    x = np.ones((a.shape[0],a.shape[1]+1))\n",
    "    x[:,:-1] = a\n",
    "    wNumber= x.shape[1]\n",
    "    numData= x.shape[0]\n",
    "    u=0\n",
    "    v=0\n",
    "    #ym es la media de todos los y\n",
    "    ym= y.mean()\n",
    "    err=0\n",
    "    for i in range(0, numData):\n",
    "        xi=x[i]\n",
    "        yi=y[i]\n",
    "        yp=(xi*w).sum()\n",
    "        #u es la suma de cuadrados residuales (yi - yPredecido)\n",
    "        u+=(yi-yp)**2\n",
    "        #u es la suma total de cuadrados (yi - yMedia)\n",
    "        v+=(yi-ym)**2\n",
    "    r2=1-(u/v)\n",
    "    return r2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evalúa el modelo con la función personalizada de éxito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14810062]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo de R^2 para el modelo 1\n",
    "r21 = calc_r2(xTest, yTest,linearRegr1)\n",
    "print(r21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09465803]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo de R^2 para el modelo 2\n",
    "r22 = calc_r2(xTest, yTest,linearRegr2)\n",
    "print(r22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13371154]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo de R^2 para el modelo 3\n",
    "r23 = calc_r2(xTest, yTest,linearRegr3)\n",
    "print(r23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "Se puede observar que el R^2 de todos los modelos es bastante bajo. Este valor es incluso más bajo que el valor de los modelos de sklearn. Al igual que en el reto 3 esto puede explicarse con el hecho que no se calculó el mínimo exacto, sino una aproximación utilizando descenso de gradiente. El modelo con menor R^2 es el tercero y el que tiene mayor R^2 es el primero, lo cual puede comprobar que los datos se comportan de una manera no lineal ya que el modelo con menos datos tuvo un mejor ajuste que el que tiene más datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
