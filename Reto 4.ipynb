{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leer el archivo con los datos\n",
    "data_wine = pd.read_csv('./data/winequality-white.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La variable dependiente es la última columna, las independientes son las anteriores\n",
    "x= data_wine.iloc[:,0:11]\n",
    "y= data_wine.iloc[:,11:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se divide el archivo para entrenamiento y test. Se reserven 10000 datos para test\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 2000, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se concatenan los datos de test\n",
    "newData= pd.concat([xTrain,yTrain], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De estos datos concatenados se escogen aleatoriamente 100,1000 y 2898 para diferentes modelos. \n",
    "#De aquí se vuelven a separar en x y y\n",
    "dataTrain1= newData.sample(100)\n",
    "xTrain1= dataTrain1.iloc[:,0:11]\n",
    "yTrain1= dataTrain1.iloc[:,11]\n",
    "dataTrain2= newData.sample(1000)\n",
    "xTrain2= dataTrain2.iloc[:,0:11]\n",
    "yTrain2= dataTrain2.iloc[:,11]\n",
    "dataTrain3= newData.sample(2898)\n",
    "xTrain3= dataTrain3.iloc[:,0:11]\n",
    "yTrain3= dataTrain3.iloc[:,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que calcula el error cuadrático medio a partir de ciertos datos y el vector de parámetros w\n",
    "def calc_errors(x,y,w):\n",
    "    err=0\n",
    "    for i in range(0,x.shape[0]):\n",
    "        err+=(y[i]-(x[i]*w).sum())**2\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que calcula el gradiente descendente, demora al correr\n",
    "#numErrors es el número de errores consecutivos cuya media debe ser menor que el threshold \n",
    "#para que se asuma que el algoritmo terminó, este se escogió con ensayo y error.\n",
    "#El threshold se define por la variable minError, este se escogió con ensayo y error.\n",
    "def gradient_desc(nX,nY,numCiclos=29,minError=0.0001):\n",
    "    a=nX.to_numpy()\n",
    "    y=nY.to_numpy()\n",
    "    #Se concatena un 1 al final de las variables para representar la constante w0\n",
    "    x = np.ones((a.shape[0],a.shape[1]+1))\n",
    "    x[:,:-1] = a\n",
    "    wNumber= x.shape[1]\n",
    "    numData= x.shape[0]\n",
    "    #Se calcula la hessiana\n",
    "    H=[[0] * wNumber]* wNumber\n",
    "    for i in range(0, numData):\n",
    "        xi= x[i]\n",
    "        H+=xi.transpose()*xi\n",
    "    ei=np.linalg.eigvals(H)\n",
    "    #Se obtienen los valores propios y se calcula n como 20/lambdaMax, valor escogido con ensayo y error. \n",
    "    #Si se utiliza 2/lambdaMax que es lo que indica la teoría el algoritmo demora mucho en converger\n",
    "    n=20/max(ei)\n",
    "    ws= [0.01] * wNumber\n",
    "    errorA=1\n",
    "    errorP=0\n",
    "    j=0\n",
    "    #Variable que representa la media mínima del error cuadrático medio actual\n",
    "    minT=10\n",
    "    #El ciclo se ejecuta hasta que el error cuadrático medio sea similar al error cuadrático medio\n",
    "    #de los numCiclos ciclos anteriores a este. Así se asume que el modelo convergió\n",
    "    while minT > minError:\n",
    "        #Se escoge una fila de datos aleatoriamente\n",
    "        i= randrange(numData)\n",
    "        #Se calcula ws con el algoritmo de descenso de gradiente estocástico\n",
    "        g=(x[i]*ws).sum()\n",
    "        e=(g-y[i])         \n",
    "        ws= ws - n*e*x[i]\n",
    "        j=(j+1)%numCiclos\n",
    "        #Cada numCiclos ciclos se pregunta si el error se similar al error de hace numCiclos ciclos\n",
    "        if j==0:\n",
    "            errorP=errorA\n",
    "            errorA=calc_errors(x,y,ws)\n",
    "            #Se guarda solo el error mínimo, que es el que se usa para comparar\n",
    "            if abs(errorP-errorA)< minT:\n",
    "                minT=abs(errorP-errorA)\n",
    "                #Se puede descomentar el siguiente print para observar como este valor disminuye\n",
    "                #print(minT)\n",
    "    return ws\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00588704 -0.06517606  0.06955145  0.03747305  0.00585837  0.00411706\n",
      " -0.00164801  0.08088695  0.29858326  0.07213041  0.42434245  0.08163437]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 1\n",
    "linearRegr1 = gradient_desc(xTrain1, yTrain1)\n",
    "print(linearRegr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15394957+0.j  0.01076167+0.j  0.01869771+0.j  0.0248744 +0.j\n",
      "  0.01031289+0.j  0.01161147+0.j -0.00206134+0.j  0.0385042 +0.j\n",
      "  0.10572849+0.j  0.02538108+0.j  0.39038961+0.j  0.03888514+0.j]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 2\n",
    "linearRegr2 = gradient_desc(xTrain2, yTrain2)\n",
    "print(linearRegr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.68711985e-01+0.j  1.41114305e-02+0.j  1.74897381e-02+0.j\n",
      "  1.25598179e-02+0.j  1.05672644e-02+0.j  1.28648492e-02+0.j\n",
      " -2.33173380e-04+0.j  3.73870625e-02+0.j  1.01314644e-01+0.j\n",
      "  2.35516821e-02+0.j  3.68180557e-01+0.j  3.77287131e-02+0.j]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 3\n",
    "linearRegr3 = gradient_desc(xTrain3, yTrain3)\n",
    "print(linearRegr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para calcular R^2 como 1-u/v, similar a como lo calcula sklearn para validar\n",
    "def calc_r2(nX,nY,w):\n",
    "    a=nX.to_numpy()\n",
    "    y=nY.to_numpy()\n",
    "    x = np.ones((a.shape[0],a.shape[1]+1))\n",
    "    x[:,:-1] = a\n",
    "    wNumber= x.shape[1]\n",
    "    numData= x.shape[0]\n",
    "    u=0\n",
    "    v=0\n",
    "    #ym es la media de todos los y\n",
    "    ym= y.mean()\n",
    "    err=0\n",
    "    for i in range(0, numData):\n",
    "        xi=x[i]\n",
    "        yi=y[i]\n",
    "        yp=(xi*w).sum()\n",
    "        #u es la suma de cuadrados residuales (yi - yPredecido)\n",
    "        u+=(yi-yp)**2\n",
    "        #u es la suma total de cuadrados (yi - yMedia)\n",
    "        v+=(yi-ym)**2\n",
    "    r2=1-(u/v)\n",
    "    return r2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19442683]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo de R^2 para el modelo 1\n",
    "r21 = calc_r2(xTest, yTest,linearRegr1)\n",
    "print(r21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1523779+0.j]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo de R^2 para el modelo 2\n",
    "r22 = calc_r2(xTest, yTest,linearRegr2)\n",
    "print(r22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11407457+0.j]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo de R^2 para el modelo 3\n",
    "r23 = calc_r2(xTest, yTest,linearRegr3)\n",
    "print(r23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "Se puede observar que el R^2 de todos los modelos es bastante bajo. Este valor es incluso más bajo que el valor de los modelos de sklearn. Al igual que en el reto 3 esto puede explicarse con el hecho que no se calculó el mínimo exacto, sino una aproximación utilizando descenso de gradiente. El modelo con menor R^2 es el tercero y el que tiene mayor R^2 es el primero, el cual por casualidad se ajustó mejor a los datos de test ya que no tenía, lo cual es probable y explica también por qué un modelo con más datos tiene peor criterio de clasificación que los demás."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
