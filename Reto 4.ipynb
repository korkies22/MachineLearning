{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se leen los datos utilizando readcsv de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leer el archivo con los datos\n",
    "data_wine = pd.read_csv('./data/winequality-white.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se separan los datos en \"x\" y \"y\". Se normaliza x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La variable dependiente es la última columna, las independientes son las anteriores\n",
    "x= data_wine.iloc[:,0:11]\n",
    "y= data_wine.iloc[:,11:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se parten los datos en entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se divide el archivo para entrenamiento y test. Se reserven 10000 datos para test\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 2000, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se concatenan los datos de test\n",
    "newData= pd.concat([xTrain,yTrain], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se escogen aleatoriamente diferentes tamaños de datos para entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De estos datos concatenados se escogen aleatoriamente 100,1000 y 2898 para diferentes modelos. \n",
    "#De aquí se vuelven a separar en x y y\n",
    "dataTrain1= newData.sample(100, random_state = 0)\n",
    "xTrain1= dataTrain1.iloc[:,0:11]\n",
    "yTrain1= dataTrain1.iloc[:,11]\n",
    "dataTrain2= newData.sample(1000, random_state = 0)\n",
    "xTrain2= dataTrain2.iloc[:,0:11]\n",
    "yTrain2= dataTrain2.iloc[:,11]\n",
    "dataTrain3= newData.sample(2898, random_state = 0)\n",
    "xTrain3= dataTrain3.iloc[:,0:11]\n",
    "yTrain3= dataTrain3.iloc[:,11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función calcula la suma de los errores cuadráticos para obtener un estimativo del error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que calcula el error cuadrático medio a partir de ciertos datos y el vector de parámetros w\n",
    "def calc_errors(x,y,w):\n",
    "    err=0\n",
    "    for i in range(0,x.shape[0]):\n",
    "        err+=(y[i]-(x[i]*w).sum())**2\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen la función necesaria para realizar descenso de gradiente estocásico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que calcula el gradiente descendente, demora al correr\n",
    "#numErrors es el número de errores consecutivos cuya media debe ser menor que el threshold \n",
    "#para que se asuma que el algoritmo terminó, este se escogió con ensayo y error.\n",
    "#El threshold se define por la variable minError, este se escogió con ensayo y error.\n",
    "def gradient_desc(nX,nY,numCiclos=29,minError=0.0001):\n",
    "    a=nX.to_numpy()\n",
    "    y=nY.to_numpy()\n",
    "    #Se concatena un 1 al final de las variables para representar la constante w0\n",
    "    x = np.ones((a.shape[0],a.shape[1]+1))\n",
    "    x[:,:-1] = a\n",
    "    wNumber= x.shape[1]\n",
    "    numData= x.shape[0]\n",
    "    #Se calcula la hessiana\n",
    "    H=[[0] * wNumber]* wNumber\n",
    "    for i in range(0, numData):\n",
    "        xi= x[i]\n",
    "        H+=xi.transpose()*xi\n",
    "    ei=np.linalg.eigvals(H)\n",
    "    #Se obtienen los valores propios y se calcula n como 20/lambdaMax, valor escogido con ensayo y error. \n",
    "    #Si se utiliza 2/lambdaMax que es lo que indica la teoría el algoritmo demora mucho en converger\n",
    "    n=20/max(ei)\n",
    "    ws= [0.01] * wNumber\n",
    "    errorA=1\n",
    "    errorP=0\n",
    "    j=0\n",
    "    #Variable que representa la media mínima del error cuadrático medio actual\n",
    "    minT=10\n",
    "    #El ciclo se ejecuta hasta que el error cuadrático medio sea similar al error cuadrático medio\n",
    "    #de los numCiclos ciclos anteriores a este. Así se asume que el modelo convergió\n",
    "    while minT > minError:\n",
    "        #Se escoge una fila de datos aleatoriamente\n",
    "        i= randrange(numData)\n",
    "        #Se calcula ws con el algoritmo de descenso de gradiente estocástico\n",
    "        g=(x[i]*ws).sum()\n",
    "        e=(g-y[i])         \n",
    "        ws= ws - n*e*x[i]\n",
    "        j=(j+1)%numCiclos\n",
    "        #Cada numCiclos ciclos se pregunta si el error se similar al error de hace numCiclos ciclos\n",
    "        if j==0:\n",
    "            errorP=errorA\n",
    "            errorA=calc_errors(x,y,ws)\n",
    "            #Se guarda solo el error mínimo, que es el que se usa para comparar\n",
    "            if abs(errorP-errorA)< minT:\n",
    "                minT=abs(errorP-errorA)\n",
    "                #Se puede descomentar el siguiente print para observar como este valor disminuye\n",
    "                #print(minT)\n",
    "    return ws\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrenan los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.89975079e-01  1.22958122e-02  1.81062079e-02 -7.81775202e-03\n",
      "  1.05618744e-02  7.21985975e-03 -1.74395681e-04  3.95074848e-02\n",
      "  1.07534485e-01  2.33471628e-02  3.74380496e-01  3.98625725e-02]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 1\n",
    "linearRegr1 = gradient_desc(xTrain1, yTrain1)\n",
    "print(linearRegr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.10442256 -0.00568421  0.01633081  0.01998139  0.0099582   0.01076732\n",
      " -0.000943    0.04427826  0.13924648  0.02861502  0.40389814  0.04474297]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 2\n",
    "linearRegr2 = gradient_desc(xTrain2, yTrain2)\n",
    "print(linearRegr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.16446946  0.01294825  0.01758163  0.0172221   0.01053063  0.0123741\n",
      " -0.00173209  0.0386495   0.10669988  0.02467593  0.38089398  0.03900927]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 3\n",
    "linearRegr3 = gradient_desc(xTrain3, yTrain3)\n",
    "print(linearRegr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye una función de éxito para evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para calcular R^2 como 1-u/v, similar a como lo calcula sklearn para validar\n",
    "def calc_r2(nX,nY,w):\n",
    "    a=nX.to_numpy()\n",
    "    y=nY.to_numpy()\n",
    "    x = np.ones((a.shape[0],a.shape[1]+1))\n",
    "    x[:,:-1] = a\n",
    "    wNumber= x.shape[1]\n",
    "    numData= x.shape[0]\n",
    "    u=0\n",
    "    v=0\n",
    "    #ym es la media de todos los y\n",
    "    ym= y.mean()\n",
    "    err=0\n",
    "    for i in range(0, numData):\n",
    "        xi=x[i]\n",
    "        yi=y[i]\n",
    "        yp=(xi*w).sum()\n",
    "        #u es la suma de cuadrados residuales (yi - yPredecido)\n",
    "        u+=(yi-yp)**2\n",
    "        #u es la suma total de cuadrados (yi - yMedia)\n",
    "        v+=(yi-ym)**2\n",
    "    r2=1-(u/v)\n",
    "    return r2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evalúa el modelo con la función personalizada de éxito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12465322]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo de R^2 para el modelo 1\n",
    "r21 = calc_r2(xTest, yTest,linearRegr1)\n",
    "print(r21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16348888]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo de R^2 para el modelo 2\n",
    "r22 = calc_r2(xTest, yTest,linearRegr2)\n",
    "print(r22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14452474]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo de R^2 para el modelo 3\n",
    "r23 = calc_r2(xTest, yTest,linearRegr3)\n",
    "print(r23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "Se puede observar que el R^2 de todos los modelos es bastante bajo. Este valor es incluso más bajo que el valor de los modelos de sklearn. Al igual que en el reto 3 esto puede explicarse con el hecho que no se calculó el mínimo exacto, sino una aproximación utilizando descenso de gradiente. El modelo con mayor R^2 es el segundo, lo cual justifica en parte que los datos se comportan de una manera no lineal ya que el modelo con menos datos tuvo un mejor ajuste que el que tiene más datos (además de ser dar como resultado todos los modelos un valor bastante bajo de R^2)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
