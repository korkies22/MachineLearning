{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randrange\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se leen los datos utilizando readcsv de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leer el archivo con los datos\n",
    "data_pulsar = pd.read_csv('./data/HTRU_2.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se separan los datos en \"x\" y \"y\". Se normaliza x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La variable dependiente es la última columna, las independientes son las anteriores.\n",
    "#Se normaliza las independientes y se convierte nuevamente en un DataFrame de Pandas\n",
    "x= pd.DataFrame.from_records(preprocessing.normalize(data_pulsar.iloc[:,0:8]))\n",
    "y= data_pulsar.iloc[:,8:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se parten los datos en entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se divide el archivo para entrenamiento y test. Se reserven 10000 datos para test\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 10000, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se concatenan los datos de test\n",
    "newData= pd.concat([xTrain,yTrain], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se escogen aleatoriamente diferentes tamaños de datos para entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De estos datos concatenados se escogen aleatoriamente 100,500,1000 y 5000 para diferentes modelos. \n",
    "#De aquí se vuelven a separar en x y y\n",
    "dataTrain1= newData.sample(100, random_state = 0)\n",
    "xTrain1= dataTrain1.iloc[:,0:8]\n",
    "yTrain1= dataTrain1.iloc[:,8]\n",
    "dataTrain2= newData.sample(500, random_state = 0)\n",
    "xTrain2= dataTrain2.iloc[:,0:8]\n",
    "yTrain2= dataTrain2.iloc[:,8]\n",
    "dataTrain3= newData.sample(1000, random_state = 0)\n",
    "xTrain3= dataTrain3.iloc[:,0:8]\n",
    "yTrain3= dataTrain3.iloc[:,8]\n",
    "dataTrain4= newData.sample(5000, random_state = 0)\n",
    "xTrain4= dataTrain4.iloc[:,0:8]\n",
    "yTrain4= dataTrain4.iloc[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen las funciones necesarias para realizar descenso de gradiente estocástico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definición de función sigmoide para usarse posteriormente\n",
    "def sigmoid(x):\n",
    "     return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que calcula w mediante gradiente ascendente\n",
    "#numErrors es el número de errores consecutivos cuya media debe ser menor que el threshold \n",
    "#para que se asuma que el algoritmo terminó.\n",
    "#El threshold se define por la variable minError, este se escogió con ensayo y error\n",
    "def gradient_asc(nX,nY,numErrors=12,minError=0.01):\n",
    "    a=nX.to_numpy()\n",
    "    y=nY.to_numpy()\n",
    "    #Se concatena un 1 al final de las variables para representar la constante w0\n",
    "    x = np.ones((a.shape[0],a.shape[1]+1))\n",
    "    x[:,:-1] = a\n",
    "    wNumber= x.shape[1]\n",
    "    numData= x.shape[0]\n",
    "    #Se calcula la hessiana\n",
    "    H=[[0] * wNumber]* wNumber\n",
    "    for i in range(0, numData):\n",
    "        xi= x[i]\n",
    "        H+=xi.transpose()*xi\n",
    "    #Se obtienen los valores propios y se calcula n como 2/lambdaMax\n",
    "    #Este valor no corresponde exactamente a este modelo ya que el error cuadrático ahora tiene una\n",
    "    #sigmoide, pero en la práctica descubrí que funciona\n",
    "    ei=np.linalg.eigvals(H)\n",
    "    n=2/max(ei)\n",
    "    ws= [0.01] * wNumber\n",
    "    error=0\n",
    "    errors=[1]* numErrors\n",
    "    j=0\n",
    "    #Variable que representa la media mínima de los últimos numErrors datos\n",
    "    minT=1\n",
    "    #Mientras la media de los últimos numErrors errores sea mayor que el error mínimo\n",
    "    while minT > minError:\n",
    "        #Se escoge una fila de datos aleatoriamente\n",
    "        i= randrange(numData)\n",
    "        #Se calcula ws con el algoritmo LMS\n",
    "        xs=(x[i]*ws).sum()\n",
    "        g=sigmoid(xs)\n",
    "        error=y[i]-g           \n",
    "        ws= ws + n*error*x[i]\n",
    "        #Se incluye en la lista de errores el último error. Como tal este no es el error de clasificación\n",
    "        #pero es un buen estimativo para saber qué tan bien el modelo se acerca a predecir los datos\n",
    "        errors[j]=abs(error)\n",
    "        #En caso de llegar al final j vuelve a ser 0 para recorrer el arreglo en orden\n",
    "        j=(j+1)%numErrors\n",
    "        #Se guarda la media mínima en caso de ser menor que la última menor\n",
    "        if minT>np.mean(errors):\n",
    "            minT=np.mean(errors)\n",
    "            #Se puede descomentar el siguiente print para observar como este valor disminuye\n",
    "            #print(minT)\n",
    "    return ws\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se calculan los modelos con las funciones anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.00049856+0.j  1.73186122+0.j  0.86602019+0.j  4.24006441+0.j\n",
      " -0.58707183+0.j  5.02740416+0.j  0.452928  +0.j -2.98775582+0.j\n",
      " -1.50180621+0.j]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 1\n",
    "logisticRegr1 = gradient_asc(xTrain1, yTrain1)\n",
    "print(logisticRegr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.14569701  1.79938709  0.97706605  4.86991026  0.39924852  5.6379321\n",
      "  0.33196886 -1.94051917 -0.78259063]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 2\n",
    "logisticRegr2 = gradient_asc(xTrain2, yTrain2)\n",
    "print(logisticRegr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.60170161+0.j  6.08325208+0.j  1.86000852+0.j  7.6428537 +0.j\n",
      " -1.85533228+0.j  8.38132332+0.j  1.29976717+0.j -1.00988986+0.j\n",
      " -1.66235154+0.j]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 3\n",
    "logisticRegr3 = gradient_asc(xTrain3, yTrain3)\n",
    "print(logisticRegr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.96386353  4.43552876  1.53210031  6.45343682 -1.64359507  8.16243716\n",
      "  0.98484873 -1.3990565  -1.42343777]\n"
     ]
    }
   ],
   "source": [
    "#Cálculo del modelo 4\n",
    "logisticRegr4 = gradient_asc(xTrain4, yTrain4)\n",
    "print(logisticRegr4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye una función de éxito para evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que calcula el error final de clasificación. \n",
    "#Como tal lo que devuelve es el porcentaje de datos exitosamente clasificados\n",
    "def calc_exito(nX,nY,w):\n",
    "    a=nX.to_numpy()\n",
    "    y=nY.to_numpy()\n",
    "    x = np.ones((a.shape[0],a.shape[1]+1))\n",
    "    x[:,:-1] = a\n",
    "    wNumber= x.shape[1]\n",
    "    numData= x.shape[0]\n",
    "    err=0\n",
    "    for i in range(0, numData):\n",
    "        prob1=sigmoid((x[i]*w).sum())\n",
    "        #El porcentaje de error aumenta si se clasificó mal tanto como 1 cómo como 0\n",
    "        if prob1>0.5 and y[i]==0:\n",
    "            err=err+1\n",
    "        #Se asigna el 0.5 como valor inclusivo hacia el 0 dado que el porcentaje de 0's es mucho mayor.\n",
    "        if prob1<=0.5 and y[i]==1:\n",
    "            err=err+1\n",
    "    return 1-(err/numData)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evalúa el modelo con la función personalizada de éxito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Se calcula el porcentaje de éxito de clasificación para el modelo 1\n",
    "exito1 = calc_exito(xTest, yTest,logisticRegr1)\n",
    "print(exito1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9637\n"
     ]
    }
   ],
   "source": [
    "#Se calcula el porcentaje de éxito de clasificación para el modelo 2\n",
    "exito2 = calc_exito(xTest, yTest,logisticRegr2)\n",
    "print(exito2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Se calcula el porcentaje de éxito de clasificación para el modelo 3\n",
    "exito3 = calc_exito(xTest, yTest,logisticRegr3)\n",
    "print(exito3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.969\n"
     ]
    }
   ],
   "source": [
    "#Se calcula el porcentaje de éxito de clasificación para el modelo 4\n",
    "exito4 = calc_exito(xTest, yTest,logisticRegr4)\n",
    "print(exito4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "Se puede observar que el porcentaje de éxito de clasificación es bastante alto. El porcentaje de éxito es casi tan alto como el de la librería sklearn. Su valor no llegó a ser el mismo posiblemente por utilizar el algoritmo de descenso de gradiente estocástico en vez de calcular el mínimo exacto. Se puede observar que el modeo con mayor porcentaje de éxito de clasificación es el primero. Esto es contraintuitivo ya que el primer modelo es el que tiene menor número de datos. Sin embargo puede que dentro de la aleatoriedad, el modelo pudo quedar con un porcentaje mayor de y=1, lo cual ayuda a que el modelo no quede sesgado hacia los 0's y no se aprenda los datos. Sin embargo, en general todos los modelos tienen un porcentaje de clasificación similar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
